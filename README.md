# TinyLLM
# ğŸ§  TinyLLM

**TinyLLM** is a minimalist character-level Transformer language model implemented in Rust. It's built as a learning project to understand how large language models (LLMs) work under the hood â€” from tokenization and training to inference.

> ğŸš€ Inspired by projects like [nanoGPT](https://github.com/karpathy/nanoGPT), but written in idiomatic Rust with a focus on clarity and control.

---

## ğŸ“¦ Features

- âœ… Pure Rust implementation â€” no Python or external ML frameworks
- âœ… Character-level tokenizer
- âœ… Transformer architecture with self-attention
- âœ… Adam optimizer with weight decay and bias correction
- âœ… Checkpointing for training state
- âœ… Simple sampling/generation loop
- âœ… Grad norm + loss tracking
- âœ… Tests for core components

---

## ğŸ› ï¸ Project Structure
src/
â”œâ”€â”€ model.rs # Transformer blocks, attention, and parameters
â”œâ”€â”€ train.rs # Training loop and Adam optimizer
â”œâ”€â”€ generate.rs # Text sampling from the trained model
â”œâ”€â”€ tokenizer.rs # Character-level tokenizer
â”œâ”€â”€ checkpoint.rs # Save/load model + optimizer state
â”œâ”€â”€ main.rs # CLI entry point (train/generate)

---

## ğŸš€ Getting Started

### 1. Clone the repo
```bash
git clone https://github.com/valicitor/tinyllm.git
cd tinyllm
```
